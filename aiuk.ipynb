{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI UK Challenge Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for taking part the Alan Turing Institute national AI showcase event, and the Data Study Group challenge. This notebook will share some details on how to get access to the data we've prepared for the event, as well as some inital analysis we've conducted, as well as some ideas on what questions you may wish to dig into during the challenge. Please feel free to come up with your own thoughts on what you'd like to work on: if you want to add secondary data, or even drop our suggested datasets and work with other public sources, go ahead!\n",
    "\n",
    "We would like each group to come up with a few interesting observations that your facilitator can discuss at the end of the event. If there's something you'd like them to talk about, whether that's a quirk of the data, some predictions based on modelling, or even an exciting approach or research field you've learned about from your group members, please let them know so they can take a note of it.\n",
    "\n",
    "**IMPORTANT**: Please check the end of this notebook for our suggestions if you're at a loss for what to start looking at, or want more guidance on what your facilitator will be talking about.\n",
    "\n",
    "Please above all else strive to remain professional and respectful during your group work, and don't worry about solving all of the world's issues in one afternoon. Remember, we're here to learn from each other, not conquer climate change at a single stroke.\n",
    "\n",
    "Good luck, and please have fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fvIZNosxt9l"
   },
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23108,
     "status": "ok",
     "timestamp": 1643990701335,
     "user": {
      "displayName": "Richard Plant",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQgA3PpckbM23VwkPmWTMqZrEMnvzoO4i_VKJ3lek=s64",
      "userId": "07329817560919253483"
     },
     "user_tz": 0
    },
    "id": "kVaM8YqvxoBU",
    "outputId": "f67c368e-0769-41ed-cad1-eaac9171b7b1"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install sklearn\n",
    "!{sys.executable} -m pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAL5L-C2xxyH"
   },
   "source": [
    "## Run setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To run this notebook in Google Colab, you need to the following first:\n",
    "1. Open this link: https://drive.google.com/drive/folders/1adprVKMxSlXTn-S3ZAbOx545cxv5CzHl?usp=sharing\n",
    "2. Then go to \"Shared with me\" in your Google Drive, right-click the \"AIUK\" folder\n",
    "and select \"Add shortcut to Drive\"\n",
    "\n",
    "Optionally, if you don't have a Google Drive account, you can set colab to False,\n",
    "and download the data.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "\n",
    "colab = False\n",
    "\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.flush_and_unmount()\n",
    "    drive.mount('/content/drive')\n",
    "    root_dir = Path('/content/drive/MyDrive/AIUK')\n",
    "else:\n",
    "    from data import download_data\n",
    "    download_data()\n",
    "    root_dir = Path('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFbTGiW7y-5Z"
   },
   "source": [
    "# Climate change indicators dataset\n",
    "\n",
    "Anthropogenic climate change is one of the most important social issues facing humanity, and one in which big data can play a role in determining the most effective policy interventions. Today, we'll be taking a look at some of the key indicators of warming, including CO2 levels, change in sea levels, and so on.\n",
    "\n",
    "Dataset source: https://datahub.io/collections/climate-change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCbO_DexG4Gz"
   },
   "source": [
    "## CO2\n",
    "\n",
    "In the next few cells we'll explore the datasets. Let's first take a look at the dataset which gives us the average monthly concentration of CO2 at the Mauna Loa observatory, measured in parts per million (PPM).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1643990707817,
     "user": {
      "displayName": "Richard Plant",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQgA3PpckbM23VwkPmWTMqZrEMnvzoO4i_VKJ3lek=s64",
      "userId": "07329817560919253483"
     },
     "user_tz": 0
    },
    "id": "O2AY_RA70fCU",
    "outputId": "eb6ee824-29c1-4c9a-e472-f5260ec58a1a"
   },
   "outputs": [],
   "source": [
    "co2_df = pd.read_csv(root_dir.joinpath('co2_monthly.csv'), parse_dates=['Date'])\n",
    "co2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9tBMNwYHnKX"
   },
   "source": [
    "As you can see, we get the date in YYYY-MM-DD format, with a monthly average, as well as a seasonally-corrected trend. Number of days refers to the number of day readings averaged, -1 denoting that the monthly data is interpolated from other sources.\n",
    "\n",
    "Let's plot the monthly average since 1990, and generate a best fit line showing the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1643990707817,
     "user": {
      "displayName": "Richard Plant",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQgA3PpckbM23VwkPmWTMqZrEMnvzoO4i_VKJ3lek=s64",
      "userId": "07329817560919253483"
     },
     "user_tz": 0
    },
    "id": "ozi1BEn0IcOR"
   },
   "outputs": [],
   "source": [
    "since_90 = co2_df[co2_df.Date>'1990']\n",
    "date_num = dates.date2num(since_90.Date)\n",
    "params = np.polyfit(date_num, since_90.Average, 1)\n",
    "fit = np.poly1d(params)\n",
    "x_fit = np.linspace(date_num.min(), date_num.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "executionInfo": {
     "elapsed": 1277,
     "status": "ok",
     "timestamp": 1643990782703,
     "user": {
      "displayName": "Richard Plant",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQgA3PpckbM23VwkPmWTMqZrEMnvzoO4i_VKJ3lek=s64",
      "userId": "07329817560919253483"
     },
     "user_tz": 0
    },
    "id": "sWHM3IR5JGrK",
    "outputId": "31755f5b-5a00-4edf-f075-fe79ca362b4a"
   },
   "outputs": [],
   "source": [
    "plt.scatter(pd.to_datetime(since_90.Date), since_90.Average, marker='.')\n",
    "plt.plot(dates.num2date(x_fit), fit(x_fit), 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31eNqXXpIV6y"
   },
   "source": [
    "## Temperature changes\n",
    "\n",
    "What about the correlation between temperature change and CO2? Let's add our global anomalous temperatures dataset to our plot as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1643990709478,
     "user": {
      "displayName": "Richard Plant",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQgA3PpckbM23VwkPmWTMqZrEMnvzoO4i_VKJ3lek=s64",
      "userId": "07329817560919253483"
     },
     "user_tz": 0
    },
    "id": "DRLxxm9FO3Be",
    "outputId": "ddc2d1b7-89cf-4ea7-a560-e8708d1656f2"
   },
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(root_dir.joinpath('temp_monthly.csv'), parse_dates=['Date'])\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1643990709478,
     "user": {
      "displayName": "Richard Plant",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQgA3PpckbM23VwkPmWTMqZrEMnvzoO4i_VKJ3lek=s64",
      "userId": "07329817560919253483"
     },
     "user_tz": 0
    },
    "id": "gJA84HcMPZ67"
   },
   "outputs": [],
   "source": [
    "temp_since_90 = temp_df.loc[(temp_df.Date > '1990') & (temp_df.Source == 'GISTEMP')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "executionInfo": {
     "elapsed": 2022,
     "status": "ok",
     "timestamp": 1643990791032,
     "user": {
      "displayName": "Richard Plant",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQgA3PpckbM23VwkPmWTMqZrEMnvzoO4i_VKJ3lek=s64",
      "userId": "07329817560919253483"
     },
     "user_tz": 0
    },
    "id": "yfBqE5uEOfam",
    "outputId": "6acfa22f-675c-470c-d6e2-a64e2c67926b"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(pd.to_datetime(temp_since_90.Date), temp_since_90.Mean, width=15.0)\n",
    "ax.set_ylabel('Anomalous temperature change (degrees C)')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel('CO2 concentration (PPM)')\n",
    "\n",
    "ax2.plot(dates.num2date(x_fit), fit(x_fit), 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20q5Bv0RUYg4"
   },
   "source": [
    "## Sea level\n",
    "\n",
    "For millions of people around the world, the most immediate impact of climate change is sea level rise, which threatens not only homes and businesses, but also vast areas of agricultural land, as well as access to clean, desalinated water.\n",
    "\n",
    "<img src=\"https://cloudfront-us-east-2.images.arcpublishing.com/reuters/3XK6LJVMXBMXZMBV45JCDUZN6Y.jpg\">\n",
    "\n",
    "\n",
    "Let's take a look at the sea level dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1643990711268,
     "user": {
      "displayName": "Richard Plant",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQgA3PpckbM23VwkPmWTMqZrEMnvzoO4i_VKJ3lek=s64",
      "userId": "07329817560919253483"
     },
     "user_tz": 0
    },
    "id": "FNxuxgWaZHxu",
    "outputId": "329f84f7-2b5b-4631-d19c-648c69a11e90"
   },
   "outputs": [],
   "source": [
    "s_df = pd.read_csv(root_dir.joinpath('sea_level.csv'), parse_dates=['Year'])\n",
    "s_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0qQKdooZ5y4"
   },
   "source": [
    "This set provides two measurements of changes in the average sea level in inches per year since 1880, provided by the CSIRO (Commonwealth Scientific and Industrial Research Organization) and EPA (United States Environmental Protection Agency). We can see where these measurements overlap by plotting the trend of each since 1990."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1643990711564,
     "user": {
      "displayName": "Richard Plant",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQgA3PpckbM23VwkPmWTMqZrEMnvzoO4i_VKJ3lek=s64",
      "userId": "07329817560919253483"
     },
     "user_tz": 0
    },
    "id": "itPn17NgZZ0b"
   },
   "outputs": [],
   "source": [
    "s_since_90 = s_df[s_df.Year > '1990']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "executionInfo": {
     "elapsed": 1283,
     "status": "ok",
     "timestamp": 1643990800185,
     "user": {
      "displayName": "Richard Plant",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjQgA3PpckbM23VwkPmWTMqZrEMnvzoO4i_VKJ3lek=s64",
      "userId": "07329817560919253483"
     },
     "user_tz": 0
    },
    "id": "9rq8BI2cZqX0",
    "outputId": "b45c8364-17e9-4e10-86c2-97781beb1d86"
   },
   "outputs": [],
   "source": [
    "plt.plot(pd.to_datetime(s_since_90.Year), s_since_90['CSIRO Adjusted Sea Level'], label='CSIRO sea level')\n",
    "plt.plot(pd.to_datetime(s_since_90.Year), s_since_90['NOAA Adjusted Sea Level'], label='NOAA sea level')\n",
    "plt.legend(loc=(0.7, 0.2), frameon=True, facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAlJdQhMfP9B"
   },
   "source": [
    "## Predicting sea level rise\n",
    "\n",
    "The rate of increase is somewhat linear, with ~9 inches higher average sea levels over the 1880 figure, but lets try to predict what the increase might look like over the next 30 years. To do that, we'll train a simple regression model.\n",
    "\n",
    "We'll first collect our data and resample our time scale to the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s_df[['Year', 'CSIRO Adjusted Sea Level']].groupby(pd.Grouper(key='Year', axis=0, freq='Y')).mean().reset_index()\n",
    "s.rename(columns = {'Year':'Date', 'CSIRO Adjusted Sea Level': 'Sea Level'}, inplace = True)\n",
    "s = s[s.Date < '2014']\n",
    "s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll train a linear regressor to predict our outcome (Sea Level) from the date. First, we'll split our available data into a training and test set, then train our regression with the labels from the training set. Then, we'll make some predictions on the test set, and see how well our model fits.\n",
    "\n",
    "**NOTE**: this is obviously not a practical solution, given that the rate of sea level rises probably isn't solely determined by the calendar year, but multiple climate-related factors, but it will work as a simple demonstration of the principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, model_selection\n",
    "import datetime\n",
    "\n",
    "X = np.array(s['Date'].apply(lambda x:x.toordinal())).reshape(-1, 1)\n",
    "y = s['Sea Level']\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "regressor = linear_model.LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "x_dt = [datetime.date.fromordinal(x[0]) for x in X_test]\n",
    "\n",
    "plt.scatter(x_dt, y_test, color=\"black\")\n",
    "plt.plot(x_dt, y_pred, color=\"blue\", linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression seems to fit quite well with the data that we've got so far, so let's make some predictions for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '2014-12-31'\n",
    "end = '2050-12-31'\n",
    "\n",
    "new_dates = pd.date_range(start, end, freq='1Y')\n",
    "x_fut = np.array(new_dates.to_series().apply(lambda x:x.toordinal())).reshape(-1, 1)\n",
    "\n",
    "y_pred = regressor.predict(x_fut)\n",
    "fut_dt = [datetime.date.fromordinal(x[0]) for x in x_fut]\n",
    "\n",
    "plt.plot(fut_dt, y_pred, color=\"blue\", linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this simple prediction, we can see that if the rate of increase stays stable, we can expect greater than 10 inches of sea level rise by 2050: [enough to put large parts of England, as well as virtually the entire Netherlands](https://coastal.climatecentral.org/map/8/1.1818/51.4022/?theme=sea_level_rise&map_type=year&basemap=roadmap&contiguous=true&elevation_model=best_available&forecast_year=2050&pathway=rcp45&percentile=p50&refresh=true&return_level=return_level_1&rl_model=gtsr&slr_model=kopp_2014), under water.\n",
    "\n",
    "![London flooding](https://2.bp.blogspot.com/_Fv90J7eTcis/TFs8VbyP1fI/AAAAAAAAGKc/tF6rECNfLbE/s1600/Flood.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other sets\n",
    "\n",
    "We have also provided the 'fossil_by_nation' dataset, which we will not explore in this notebook. If you're interested in analysing the distribution of fossil fuel usage by nation states over time, we encourage you to explore this dataset.\n",
    "\n",
    "Here are the column headers to get you started.\n",
    "\n",
    "| Column Name | Contents |\n",
    "|---|---|\n",
    "| Year | 4-digit year |\n",
    "| Country | Country as an uppercased string (e.g. UNITED KINGDOM) |\n",
    "| Total | Total carbon emissions from fossil fuel consumption and cement production (million metric tons of C) |\n",
    "| Solid Fuel | Carbon emissions from solid fuel consumption |\n",
    "| Liquid Fuel | Carbon emissions from liquid fuel consumption |\n",
    "| Gas Fuel | Carbon emissions from gas fuel consumption |\n",
    "| Cement | Carbon emissions from cement production |\n",
    "| Gas Flaring | Carbon emissions from gas flaring |\n",
    "| Per Capita | Per capita carbon emissions (metric tons of carbon; after 1949 only) |\n",
    "| Bunker Fuels | Carbon emissions from bunker fuels (not included in total) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yg7sook3OTS"
   },
   "source": [
    "# Climate change sentiment dataset\n",
    "\n",
    "Beyond the bare numbers, combating climate change will require buy-in from people in all walks of life. Part of persuading people about the need to combat the deadly outcomes of global warming is identifying which parts of the population are already persuaded, and what evidence each side is deploying to support their point of view.\n",
    "\n",
    "We've provided a dataset includes a large group of social media posts related to climate change, annotated by human experts as referring to either factual news sources (2), or as opinion indicating that the person is promoting either belief or disbelief in anthropogenic warming (1 or -1). Messages without an obvious valence are marked with a 0.\n",
    "\n",
    "You can find that dataset in your Google Drive at /content/drive/MyDrive/AIUK/cc_sentiment.csv\n",
    "\n",
    "Dataset source: https://www.kaggle.com/edqian/twitter-climate-change-sentiment-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(root_dir.joinpath('cc_sentiment.csv'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many examples of each class we have in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data['sentiment'].value_counts().sort_index().values\n",
    "\n",
    "plt.bar(range(-1, 3), counts, tick_label=range(-1, 3))\n",
    "\n",
    "for index, value in enumerate(counts):\n",
    "    plt.text(index - 1.05, value + 300, str(value))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our dataset is quite imbalanced: the vast majority of the examples we have express belief in anthropogenic climate change, while a very small minority hold the opposite position, or are neutral. This might have some severe effects on machine learning models we might want to train with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning with text\n",
    "\n",
    "Let's explore that idea by training a simple natural language processing (NLP) model to predict whether a message text holds either belief. \n",
    "\n",
    "We'll stick with some straightforward principles: first, we'll treat each message as a bag of words, that is we will ignore the order and structure of words, and just count how many times each appears. Second, we'll use a Decision Tree model to make decisions about what belief each message demonstrates, given the words that appear in it.\n",
    "\n",
    "Let's go ahead and prepare our training and test data by including only messages annotated with either 1 or -1, then converting each message text to a list of word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.loc[(data.sentiment == 1) | (data.sentiment == -1)]\n",
    "X = df.message\n",
    "Y = df.sentiment\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(strip_accents='unicode', stop_words='english')\n",
    "X_counts = count_vect.fit_transform(X)\n",
    "\n",
    "# Feature matrix shape\n",
    "X_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a set of feature vectors and labels, we can go ahead and split that into training and test sets, and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_counts, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of nearly 90% looks quite good, right? Well, that number may be giving us the wrong impression. Let's look at some more scoring metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, preds)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the classifier is almost always predicting that the message shows belief in anthropogenic climate change, regardless of what the message content looks like. Because our dataset is so imbalanced, with by far the majority of examples in the positive class, this means that most examples are going to be classified correctly by default, meaning that the accuracy score will be high, regardless of what our model is actually learning.\n",
    "\n",
    "Using another metric which takes account of the ratio of false predictions as well as true predictions, like F1 or the Matthews Correlation Coefficient, gives us a much clearer understanding of what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsYJrkxUDiKj"
   },
   "source": [
    "# Ideas for discussion\n",
    "\n",
    "Here's a set of questions that you might like to think about answering during your DSG challenge. If you don't see anything you're interested in answering, don't worry - feel free to come up with your own problem to solve or interesting piece of data to dig up.\n",
    "\n",
    "*  Which countries have historically emitted a lot of co2, and what has changed over time?\n",
    "*  Extrapolate from the cc data to see what potential changes could do to temperature/atmospheric co2?\n",
    "*  What topics do people who believe in human-caused climate change talk about, versus those who don't?\n",
    "*  Do believers/non-believers form networks - do they retweet each other a lot? You might have to think about linking accounts through matching message texts, or using retweets.\n",
    "*  Can you predict from a message text whether a person is going to believe in anthropogenic climate change better than the simple model we created? \n",
    "*  What about if you disregard named entities, like the UN, IPCC, and so on? What does that do to your models' performance?\n",
    "* The class imbalance in the text dataset is quite bad, can you determine a way to address this to make machine learning easier to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y66uEfcKDlUO"
   },
   "source": [
    "# Presenting your work\n",
    "\n",
    "The final outcome of your DSG challenge will be a very short presentation delivered by your facilitator. We're looking for something that can provide the following:\n",
    "\n",
    "* An interesting finding for discussion, which you can illustrate with a single slide\n",
    "* A five minute slot for talking about your experiences during the challenge\n",
    "\n",
    "That last point is important: we want this to be a collaborative experience, so be prepared that your facilitator might talk about some of the things you come up with during the event. Remember, this event is for the benefit of all participants, so remain polite, collegial, and constructive at all times."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPhDMC6rhRPwb3F1P5CsTQt",
   "collapsed_sections": [],
   "name": "aiuk_trial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aiuk",
   "language": "python",
   "name": "aiuk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
